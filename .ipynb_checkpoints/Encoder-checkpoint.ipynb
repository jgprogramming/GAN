{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy\n",
    "import sklearn.datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_data = np.reshape(train_data, [-1, 28, 28, 1])\n",
    "\n",
    "input_data = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "resize = tf.image.resize_images(\n",
    "    input_data,\n",
    "    [32, 32]\n",
    ")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_data =  sess.run(resize, feed_dict={input_data: train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(noise, message):\n",
    "    input_layer = tf.contrib.layers.flatten(tf.concat([tf.cast(noise, tf.float32), tf.cast(message, tf.float32)],1))\n",
    "    dense_1 = tf.layers.dense(inputs=input_layer, units=1024, name=\"g_dense_1\")\n",
    "    reshape_1 = tf.reshape(dense_1, shape=[-1, 32, 32, 1])\n",
    "    \n",
    "    conv_1 = tf.layers.conv2d_transpose(\n",
    "      inputs=reshape_1,\n",
    "      filters=64,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      name=\"g_conv_1\")\n",
    "    \n",
    "    conv_2 = tf.layers.conv2d_transpose(\n",
    "      inputs=conv_1,\n",
    "      filters=16,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      name=\"g_conv_2\")\n",
    "   \n",
    "    conv_3 = tf.layers.conv2d_transpose(\n",
    "      inputs=conv_2,\n",
    "      filters=4,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      name=\"g_conv_3\")\n",
    "\n",
    "    conv_4 = tf.layers.conv2d_transpose(\n",
    "      inputs=conv_3,\n",
    "      filters=1,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      name=\"g_conv_4\",\n",
    "      activation=tf.nn.tanh)\n",
    "        \n",
    "    return conv_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(input_batch):\n",
    "    pool_1 = tf.layers.average_pooling2d(inputs=input_batch, pool_size=[4, 4], strides=4)\n",
    "    return tf.contrib.layers.flatten(pool_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(input_batch, reuse=False):\n",
    "    conv_1 = tf.layers.conv2d(\n",
    "      inputs=input_batch,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      name=\"d_conv_1\",\n",
    "      reuse=reuse)\n",
    "        \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv_1, pool_size=[2, 2], strides=2)\n",
    "        \n",
    "    conv_2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      name=\"d_conv_2\",\n",
    "      reuse=reuse)\n",
    "        \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv_2, pool_size=[2, 2], strides=2)\n",
    "        \n",
    "    dense_1 = tf.layers.dense(inputs=pool2, units=1024, name=\"d_dense_1\", activation=tf.nn.relu, reuse=reuse)\n",
    "    \n",
    "    dropout = tf.nn.dropout(dense_1, 0.4)\n",
    "    \n",
    "    dense_2 = tf.layers.dense(inputs=dense_1, units=1, name=\"d_dense_2\", reuse=reuse)\n",
    "\n",
    "    return dense_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer g_conv_1 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 192]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a34df00ebcd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-9c205e65a368>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(noise, message)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       name=\"g_conv_1\")\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     conv_2 = tf.layers.conv2d_transpose(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m   1788\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m       _scope=name)\n\u001b[0;32m-> 1790\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \"\"\"\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1179\u001b[0m                            \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m                            str(x.get_shape().as_list()))\n\u001b[0m\u001b[1;32m   1182\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer g_conv_1 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 192]"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "noise = tf.placeholder(\"float\", [None, 128])\n",
    "message = tf.placeholder(\"float\", [None, 64])\n",
    "real_images = tf.placeholder(\"float\", [None, 32, 32, 1])\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10000\n",
    "gen = generator(noise, message)\n",
    "dec = decoder(gen)\n",
    "\n",
    "Dreal = classifier(real_images) #Dx will hold discriminator prediction probabilities for the real MNIST images\n",
    "Dfake = classifier(gen, reuse=True) #Dg will hold discriminator prediction probabilities for generated images\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dfake, labels = tf.ones_like(Dfake)))\n",
    "# message_loss = tf.losses.mean_squared_error(dec, message)  + g_loss\n",
    "message_loss = g_loss\n",
    "\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dreal, labels = tf.ones_like(Dreal)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dfake, labels = tf.zeros_like(Dfake)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "vars = tf.trainable_variables()\n",
    "gen_vars = [v for v in vars if v.name.startswith('g_')]\n",
    "disc_vars = [v for v in vars if v.name.startswith('d_')]\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "gen_train_op = optimizer.minimize(message_loss, var_list=gen_vars)\n",
    "disc_train_op = optimizer.minimize(d_loss, var_list=disc_vars)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        noise_batch = np.random.uniform(-1,1,[BATCH_SIZE, 128])\n",
    "        message_batch = np.random.randint(2, size=[BATCH_SIZE, 64]) / 2 + 0.25\n",
    "        real_images_batch = np.array([ train_data[i] for i in sorted(random.sample(range(len(train_data)), BATCH_SIZE)) ])\n",
    "        d_loss_out = 0\n",
    "        d_loss_out, _1 = sess.run([d_loss, disc_train_op], feed_dict={noise: noise_batch, message: message_batch, real_images: real_images_batch})\n",
    "        out, code, loss, _1 =  sess.run([gen, dec, message_loss, gen_train_op], feed_dict={noise: noise_batch, message: message_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, \": g_loss: \", loss, \" , desc_loss: \", d_loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf9f2f8940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index1, index2 = 12, 26\n",
    "print(code[index1][index2])\n",
    "print(message_batch[index1][index2])\n",
    "imgs = out.reshape((-1, 32, 32))\n",
    "plt.imshow(imgs[index1], cmap='gray')\n",
    "plt.imshow(imgs[index1], cmap='gray')\n",
    "# def binary(arr):\n",
    "#     out = np.copy(arr)\n",
    "#     out[arr > 0.5] = 1.0\n",
    "#     out[arr <= 0.5] = 0.0\n",
    "#     return out\n",
    "    \n",
    "# def minus(one, two):\n",
    "#     arr = binary(one) - binary(two)\n",
    "#     arr[arr != 0] = 1\n",
    "#     return sum(arr)\n",
    "\n",
    "# for i in range(0, len(code)):\n",
    "#     print(minus(message_batch[i], code[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
